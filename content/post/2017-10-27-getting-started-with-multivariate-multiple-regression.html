---
title: Getting started with Multivariate Multiple Regression
author: Clay Ford
date: '2017-10-27'
slug: getting-started-with-multivariate-multiple-regression
categories:
  - R
tags:
  - multivariate multiple regression
  - manova
---



<p>Multivariate Multiple Regression is the method of modeling multiple responses, or dependent variables, with a single set of predictor variables. For example, we might want to model both math and reading SAT scores as a function of gender, race, parent income, and so forth. This allows us to evaluate the relationship of, say, gender with each score. You may be thinking, “why not just run separate regressions for each dependent variable?” That’s actually a good idea! And in fact that’s pretty much what multivariate multiple regression does. It regresses each dependent variable separately on the predictors. However, because we have multiple responses, we have to modify our hypothesis tests for regression parameters and our confidence intervals for predictions.</p>
<p>To get started, let’s read in some data from the book <em>Applied Multivariate Statistical Analysis (6th ed.)</em> by Richard Johnson and Dean Wichern. This data come from exercise 7.25 and involve 17 overdoses of the drug amitriptyline (Rudorfer, 1982). There are two responses we want to model: TOT and AMI. TOT is total TCAD plasma level and AMI is the amount of amitriptyline present in the TCAD plasma level. The predictors are as follows:</p>
<ul>
<li>GEN, gender (male = 0, female = 1)</li>
<li>AMT, amount of drug taken at time of overdose</li>
<li>PR, <a href="https://en.wikipedia.org/wiki/PR_interval" target="_blank">PR wave measurement</a></li>
<li>DIAP, diastolic blood pressure</li>
<li>QRS, <a href="https://en.wikipedia.org/wiki/QRS_complex" target="_blank">QRS wave measurement</a></li>
</ul>
<p>We’ll use the <a href="https://www.r-project.org/" target="_blank">R statistical computing environment</a> to demonstrate multivariate multiple regression. The following code reads the data into R and names the columns.</p>
<pre class="r"><code>ami_data &lt;- read.table(&quot;http://static.lib.virginia.edu/statlab/materials/data/ami_data.DAT&quot;)
names(ami_data) &lt;- c(&quot;TOT&quot;,&quot;AMI&quot;,&quot;GEN&quot;,&quot;AMT&quot;,&quot;PR&quot;,&quot;DIAP&quot;,&quot;QRS&quot;)</code></pre>
<pre class="r"><code>summary(ami_data)
pairs(ami_data)</code></pre>
<p>Performing multivariate multiple regression in R requires wrapping the multiple responses in the <code>cbind</code> function. <code>cbind</code> takes two vectors, or columns, and “binds” them together into two columns of data. We insert that on the left side of the formula operator: <code>~</code>. On the other side we add our predictors. The <code>+</code> signs do not mean addition per se but rather inclusion. Taken together the formula <code>cbind(TOT, AMI) ~ GEN + AMT + PR + DIAP + QRS</code> translates to “model TOT and AMI as a function of GEN, AMT, PR, DIAP and QRS.” To fit this model we use the workhorse <code>lm</code> function and save it to an object we named “mlm1”. Finally we view the results with <code>summary</code>.</p>
<pre class="r"><code>mlm1 &lt;- lm(cbind(TOT, AMI) ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(mlm1)</code></pre>
<pre><code>## Response TOT :
## 
## Call:
## lm(formula = TOT ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -399.2 -180.1    4.5  164.1  366.8 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.879e+03  8.933e+02  -3.224 0.008108 ** 
## GEN          6.757e+02  1.621e+02   4.169 0.001565 ** 
## AMT          2.848e-01  6.091e-02   4.677 0.000675 ***
## PR           1.027e+01  4.255e+00   2.414 0.034358 *  
## DIAP         7.251e+00  3.225e+00   2.248 0.046026 *  
## QRS          7.598e+00  3.849e+00   1.974 0.074006 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 281.2 on 11 degrees of freedom
## Multiple R-squared:  0.8871, Adjusted R-squared:  0.8358 
## F-statistic: 17.29 on 5 and 11 DF,  p-value: 6.983e-05
## 
## 
## Response AMI :
## 
## Call:
## lm(formula = AMI ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -373.85 -247.29  -83.74  217.13  462.72 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.729e+03  9.288e+02  -2.938 0.013502 *  
## GEN          7.630e+02  1.685e+02   4.528 0.000861 ***
## AMT          3.064e-01  6.334e-02   4.837 0.000521 ***
## PR           8.896e+00  4.424e+00   2.011 0.069515 .  
## DIAP         7.206e+00  3.354e+00   2.149 0.054782 .  
## QRS          4.987e+00  4.002e+00   1.246 0.238622    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 292.4 on 11 degrees of freedom
## Multiple R-squared:  0.8764, Adjusted R-squared:  0.8202 
## F-statistic:  15.6 on 5 and 11 DF,  p-value: 0.0001132</code></pre>
<p>Notice the summary shows the results of two regressions: one for TOT and one for AMI. These are exactly the same results we would get if modeled each separately. You can verify this for yourself by running the following code and comparing the summaries to what we got above. They’re identical.</p>
<pre class="r"><code>m1 &lt;- lm(TOT ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(m1)
m2 &lt;- lm(AMI ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(m2)</code></pre>
<p>The same diagnostics we check for models with one predictor should be checked for these as well. For a review of some basic but essential diagnostics see our post <a href="http://data.library.virginia.edu/diagnostic-plots/" target="_blank">Understanding Diagnostic Plots for Linear Regression Analysis</a>.</p>
<p>We can use R’s extractor functions with our mlm1 object, except we’ll get double the output. For example, instead of one set of residuals, we get two:</p>
<pre class="r"><code>head(resid(mlm1))</code></pre>
<pre><code>##          TOT        AMI
## 1  132.82172  161.52769
## 2  -72.00392 -264.35329
## 3 -399.24769 -373.85244
## 4 -382.84730 -247.29456
## 5 -152.39129   15.78777
## 6  366.78644  217.13206</code></pre>
<p>Instead of one set of fitted values, we get two:</p>
<pre class="r"><code>head(fitted(mlm1))</code></pre>
<pre><code>##         TOT       AMI
## 1 3256.1783 2987.4723
## 2 1173.0039  917.3533
## 3 1530.2477 1183.8524
## 4  978.8473  695.2946
## 5 1048.3913  828.2122
## 6 1400.2136 1232.8679</code></pre>
<p>Instead of one set of coefficients, we get two:</p>
<pre class="r"><code>coef(mlm1)</code></pre>
<pre><code>##                       TOT           AMI
## (Intercept) -2879.4782461 -2728.7085444
## GEN           675.6507805   763.0297617
## AMT             0.2848511     0.3063734
## PR             10.2721328     8.8961977
## DIAP            7.2511714     7.2055597
## QRS             7.5982397     4.9870508</code></pre>
<p>Again these are all identical to what we get by running separate models for each response. The similarity ends, however, with the variance-covariance matrix of the model coefficients. We don’t reproduce the output here because of the size, but we encourage you to view it for yourself:</p>
<pre class="r"><code>vcov(mlm1)</code></pre>
<p>The main takeaway is that the coefficients from both models covary. That covariance needs to be taken into account when determining if a predictor is jointly contributing to both models. For example, the effects of PR and DIAP seem borderline. They appear significant for TOT but less so for AMI. But it’s not enough to eyeball the results from the two separate regressions! We need to formally test for their inclusion. And that test involves the covariances between the coefficients in both models.</p>
<p>Determining whether or not to include predictors in a multivariate multiple regression requires the use of multivariate test statistics. These are often taught in the context of MANOVA, or multivariate analysis of variance. Again the term “multivariate” here refers to multiple responses or dependent variables. This means we use modified hypothesis tests to determine whether a predictor contributes to a model.</p>
<p>The easiest way to do this is to use the <code>Anova</code> or <code>Manova</code> functions in the <code>car</code> package (Fox and Weisberg, 2011), like so:</p>
<pre class="r"><code>library(car)
Anova(mlm1)</code></pre>
<pre><code>## 
## Type II MANOVA Tests: Pillai test statistic
##      Df test stat approx F num Df den Df   Pr(&gt;F)   
## GEN   1   0.65521   9.5015      2     10 0.004873 **
## AMT   1   0.69097  11.1795      2     10 0.002819 **
## PR    1   0.34649   2.6509      2     10 0.119200   
## DIAP  1   0.32381   2.3944      2     10 0.141361   
## QRS   1   0.29184   2.0606      2     10 0.178092   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The results are titled “Type II MANOVA Tests”. The <code>Anova</code> function automatically detects that mlm1 is a multivariate multiple regression object. “Type II” refers to the type of sum-of-squares. This basically says that predictors are tested assuming all other predictors are already in the model. This is usually what we want. Notice that PR and DIAP appear to be jointly insignificant for the two models despite what we were led to believe by examining each model separately.</p>
<p>Based on these results we may want to see if a model with just GEN and AMT fits as well as a model with all five predictors. One way we can do this is to fit a smaller model and then compare the smaller model to the larger model using the <code>anova</code> function, (notice the little “a”; this is different from the <code>Anova</code> function in the <code>car</code> package). For example, below we create a new model using the <code>update</code> function that only includes GEN and AMT. The expression <code>. ~ . - PR - DIAP - QRS</code> says “keep the same responses and predictors except PR, DIAP and QRS.”</p>
<pre class="r"><code>mlm2 &lt;- update(mlm1, . ~ . - PR - DIAP - QRS)
anova(mlm1, mlm2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: cbind(TOT, AMI) ~ GEN + AMT + PR + DIAP + QRS
## Model 2: cbind(TOT, AMI) ~ GEN + AMT
##   Res.Df Df Gen.var.  Pillai approx F num Df den Df Pr(&gt;F)
## 1     11       43803                                      
## 2     14  3    51856 0.60386   1.5859      6     22 0.1983</code></pre>
<p>The large p-value provides good evidence that the model with two predictors fits as well as the model with five predictors. Notice the test statistic is “Pillai”, which is one of the four common multivariate test statistics.</p>
<p>The <code>car</code> package provides another way to conduct the same test using the <code>linearHypothesis</code> function. The beauty of this function is that it allows us to run the test without fitting a separate model. It also returns all four multivariate test statistics. The first argument to the function is our model. The second argument is our null hypothesis. The <code>linearHypothesis</code> function conveniently allows us to enter this hypothesis as character phrases. The null entered below is that the coefficients for PR, DIAP and QRS are all 0.</p>
<pre class="r"><code>lh.out &lt;- linearHypothesis(mlm1, hypothesis.matrix = c(&quot;PR = 0&quot;, &quot;DIAP = 0&quot;, &quot;QRS = 0&quot;))
lh.out</code></pre>
<pre><code>## 
## Sum of squares and products for the hypothesis:
##          TOT      AMI
## TOT 930348.1 780517.7
## AMI 780517.7 679948.4
## 
## Sum of squares and products for error:
##          TOT      AMI
## TOT 870008.3 765676.5
## AMI 765676.5 940708.9
## 
## Multivariate Tests: 
##                  Df test stat approx F num Df den Df  Pr(&gt;F)  
## Pillai            3 0.6038599 1.585910      6     22 0.19830  
## Wilks             3 0.4405021 1.688991      6     20 0.17553  
## Hotelling-Lawley  3 1.1694286 1.754143      6     18 0.16574  
## Roy               3 1.0758181 3.944666      3     11 0.03906 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The Pillai result is the same as we got using the <code>anova</code> function above. The Wilks, Hotelling-Lawley, and Roy results are different versions of the same test. The consensus is that the coefficients for PR, DIAP and QRS do not seem to be statistically different from 0. There is some discrepancy in the test results. The Roy test in particular is significant, but this is likely due to the small sample size (n = 17).</p>
<p>Also included in the output are two sum of squares and products matrices, one for the hypothesis and the other for the error. These matrices are used to calculate the four test statistics. These matrices are stored in the lh.out object as SSPH (hypothesis) and SSPE (error). We can use these to manually calculate the test statistics. For example, let SSPH = H and SSPE = E. The formula for the Wilks test statistic is</p>
<p><span class="math display">\[
\frac{\begin{vmatrix}\bf{E}\end{vmatrix}}{\begin{vmatrix}\bf{E} + \bf{H}\end{vmatrix}}
\]</span></p>
<p>In R we can calculate that as follows:</p>
<pre class="r"><code>E &lt;- lh.out$SSPE
H &lt;- lh.out$SSPH
det(E)/det(E + H)</code></pre>
<pre><code>## [1] 0.4405021</code></pre>
<p>Likewise the formula for Pillai is</p>
<p><span class="math display">\[
tr[\bf{H}(\bf{H} + \bf{E})^{-1}]
\]</span></p>
<p>tr means trace. That’s the sum of the diagonal elements of a matrix. In R we can calculate as follows:</p>
<pre class="r"><code>sum(diag(H %*% solve(E + H)))</code></pre>
<pre><code>## [1] 0.6038599</code></pre>
<p>The formula for Hotelling-Lawley is</p>
<p><span class="math display">\[
tr[\bf{H}\bf{E}^{-1}]
\]</span></p>
<p>In R:</p>
<pre class="r"><code>sum(diag(H %*% solve(E)))</code></pre>
<pre><code>## [1] 1.169429</code></pre>
<p>And finally the Roy statistic is the largest eigenvalue of <span class="math inline">\(\bf{H}\bf{E}^{-1}\)</span>.</p>
<p>In R code:</p>
<pre class="r"><code>e.out &lt;- eigen(H %*% solve(E))
max(e.out$values)</code></pre>
<pre><code>## [1] 1.075818</code></pre>
<p>Given these test results, we may decide to drop PR, DIAP and QRS from our model. In fact this is model mlm2 that we fit above. Here is the summary:</p>
<pre class="r"><code>summary(mlm2)</code></pre>
<pre><code>## Response TOT :
## 
## Call:
## lm(formula = TOT ~ GEN + AMT, data = ami_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -756.05 -190.68  -59.83  203.32  560.84 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  56.72005  206.70337   0.274   0.7878    
## GEN         507.07308  193.79082   2.617   0.0203 *  
## AMT           0.32896    0.04978   6.609 1.17e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 358.6 on 14 degrees of freedom
## Multiple R-squared:  0.7664, Adjusted R-squared:  0.733 
## F-statistic: 22.96 on 2 and 14 DF,  p-value: 3.8e-05
## 
## 
## Response AMI :
## 
## Call:
## lm(formula = AMI ~ GEN + AMT, data = ami_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -716.80 -135.83  -23.16  182.27  695.97 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -241.34791  196.11640  -1.231  0.23874    
## GEN          606.30967  183.86521   3.298  0.00529 ** 
## AMT            0.32425    0.04723   6.866 7.73e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 340.2 on 14 degrees of freedom
## Multiple R-squared:  0.787,  Adjusted R-squared:  0.7566 
## F-statistic: 25.87 on 2 and 14 DF,  p-value: 1.986e-05</code></pre>
<p>Now let’s say we wanted to use this model to predict TOT and AMI for GEN = 1 (female) and AMT = 1200. We can use the <code>predict</code> function for this. First we need put our new data into a data frame with column names that match our original data.</p>
<pre class="r"><code>nd &lt;- data.frame(GEN = 1, AMT = 1200)
p&lt;- predict(mlm2, nd)
p</code></pre>
<pre><code>##        TOT      AMI
## 1 958.5473 754.0677</code></pre>
<p>This predicts two values, one for each response. Now this is just a prediction and has uncertainty. We usually quantify uncertainty with confidence intervals to give us some idea of a lower and upper bound on our estimate. But in this case we have two predictions from a multivariate model with two sets of coefficients that covary! This means calculating a confidence interval is more difficult. In fact we don’t calculate an interval but rather an ellipse to capture the uncertainty in two dimensions.</p>
<p>Unfortunately at the time of this writing there doesn’t appear to be a function in R for creating uncertainty ellipses for multivariate multiple regression models with two responses. However we have written one below you can use called <code>predictionEllipse</code>. The details of the function go beyond a “getting started” blog post but it should be easy enough to use. Simply submit the code in the console to create the function. Then use the function with any multivariate multiple regression model object that has two responses. The newdata argument works the same as the newdata argument for predict. Use the level argument to specify a confidence level between 0 and 1. The default is 0.95. Set ggplot to FALSE to create the plot using base R graphics.</p>
<pre class="r"><code>predictionEllipse &lt;- function(mod, newdata, level = 0.95, ggplot = TRUE){
  # labels
  lev_lbl &lt;- paste0(level * 100, &quot;%&quot;)
  resps &lt;- colnames(mod$coefficients)
  title &lt;- paste(lev_lbl, &quot;confidence ellipse for&quot;, resps[1], &quot;and&quot;, resps[2])
  
  # prediction
  p &lt;- predict(mod, newdata)
  
  # center of ellipse
  cent &lt;- c(p[1,1],p[1,2])
  
  # shape of ellipse
  Z &lt;- model.matrix(mod)
  Y &lt;- mod$model[[1]]
  n &lt;- nrow(Y)
  m &lt;- ncol(Y)
  r &lt;- ncol(Z) - 1
  S &lt;- crossprod(resid(mod))/(n-r-1)
  
  # radius of circle generating the ellipse
  tt &lt;- terms(mod)
  Terms &lt;- delete.response(tt)
  mf &lt;- model.frame(Terms, newdata, na.action = na.pass, 
                   xlev = mod$xlevels)
  z0 &lt;- model.matrix(Terms, mf, contrasts.arg = mod$contrasts)
  rad &lt;- sqrt((m*(n-r-1)/(n-r-m))*qf(level,m,n-r-m)*z0%*%solve(t(Z)%*%Z) %*% t(z0))
  
  # generate ellipse using ellipse function in car package
  ell_points &lt;- car::ellipse(center = c(cent), shape = S, radius = c(rad), draw = FALSE)
  
  # ggplot2 plot
  if(ggplot){
    require(ggplot2, quietly = TRUE)
    ell_points_df &lt;- as.data.frame(ell_points)
    ggplot(ell_points_df, aes(x, y)) +
      geom_path() +
      geom_point(aes(x = TOT, y = AMI), data = data.frame(p)) +
      labs(x = resps[1], y = resps[2], 
           title = title)
  } else {
    # base R plot
    plot(ell_points, type = &quot;l&quot;, xlab = resps[1], ylab = resps[2], main = title)
    points(x = cent[1], y = cent[2])
  }
}</code></pre>
<p>Here’s a demonstration of the function.</p>
<pre class="r"><code>predictionEllipse(mod = mlm2, newdata = nd)</code></pre>
<p><img src="/post/2017-10-27-getting-started-with-multivariate-multiple-regression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The dot in the center is our predicted values for TOT and AMI. The ellipse represents the uncertainty in this prediction. We’re 95% confident the true values of TOT and AMI when GEN = 1 and AMT = 1200 are within the area of the ellipse. Notice also that TOT and AMI seem to be positively correlated. Predicting higher values of TOT means predicting higher values of AMI, and vice versa.</p>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li>Fox, J and Weisberg, S (2011). <em>An {R} Companion to Applied Regression, Second Edition</em>. Thousand Oaks CA: Sage. URL: <a href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion" target="_blank">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</a></li>
<li>Johnson, R and Wichern, D (2007). <em>Applied Multivariate Statistical Analysis, Sixth Edition</em>. Prentice-Hall.</li>
<li>Rudorfer, MV “Cardiovascular Changes and Plasma Drug Levels after Amitriptyline Overdose.” <em>Journal of Toxicology-Clinical Toxicology</em>, 19 (1982), 67-71.</li>
</ul>
<p>For questions or clarifications regarding this article, contact the UVa Library StatLab: <a href="mailto:statlab@virginia.edu">statlab@virginia.edu</a></p>
<p><em>Clay Ford</em><br />
<em>Statistical Research Consultant</em><br />
<em>University of Virginia Library</em></p>
</div>
